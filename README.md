<p align="center">
  <h1 align="center">Retrieval-Augmented Generation pipeline</h1>
  <p align="center">
    <strong>Build a production-ready Retrieval-Augmented Generation pipeline from the ground up.</strong>
  </p>
  <p align="center">
    <a href="#-quick-start">Quick Start</a> â€¢
    <a href="#-architecture">Architecture</a> â€¢
    <a href="#-project-structure">Project Structure</a> â€¢
    <a href="#-api-reference">API Reference</a> â€¢
    <a href="#-configuration">Configuration</a> â€¢
    <a href="#-docker">Docker</a> â€¢
    <a href="#-contributing">Contributing</a>
  </p>
  <p align="center">
    <img src="https://img.shields.io/badge/python-3.10%2B-blue?logo=python&logoColor=white" alt="Python 3.10+">
    <img src="https://img.shields.io/badge/FastAPI-0.109+-009688?logo=fastapi&logoColor=white" alt="FastAPI">
    <img src="https://img.shields.io/badge/Redis-7.0+-DC382D?logo=redis&logoColor=white" alt="Redis">
    <img src="https://img.shields.io/badge/Ollama-3.2+-000000?logo=ollama&logoColor=white" alt="Ollama">
    <img src="https://img.shields.io/badge/license-MIT-green" alt="MIT License">
  </p>
</p>

---

## Overview

**RAG from Scratch** is a modular, extensible template for building Retrieval-Augmented Generation systems without relying on black-box frameworks. Every component â€” from document loading to LLM generation â€” is implemented from first principles, giving you full control and understanding of the pipeline.

### Key Features

| Feature | Description |
|---|---|
| **Multi-Format Ingestion** | Load PDF, DOCX, XLSX, PPTX, TXT, and Markdown files |
| **Smart Chunking** | Recursive character-aware text splitting with configurable overlap |
| **Local Embeddings** | Sentence-Transformers (`all-MiniLM-L6-v2`) â€” no API keys required |
| **Dual Vector Store** | In-memory store for development, Redis for production |
| **Semantic Retrieval** | Cosine-similarity search with top-k and metadata filtering |
| **Local LLM** | Ollama integration (Llama 3.2, Mistral, etc.) â€” fully offline |
| **Caching Layer** | Redis-backed embedding and query cache with configurable TTL |
| **REST API** | FastAPI server with document ingestion, querying, and admin endpoints |
| **Docker Ready** | Multi-stage Dockerfile for containerized deployments |

---

## Architecture

### System Architecture Diagram (generated using gemini)

![RAG Architecture](./images/rag_architecture.png)

The diagram above illustrates the complete RAG pipeline architecture, showing:
- **Ingestion Flow** (left): Document loading â†’ Text splitting â†’ Embedding â†’ Vector storage
- **Query Flow** (right): User query â†’ Embedding â†’ Similarity search â†’ Context retrieval â†’ LLM generation
- **Infrastructure**: Redis for vector storage and caching, Ollama for local LLM inference

---

### Data Flow Sequence

```mermaid
sequenceDiagram
    participant U as ðŸ‘¤ User
    participant API as âš¡ FastAPI
    participant IS as IngestionService
    participant RS as RetrievalService
    participant VS as VectorStore (Redis)
    participant LLM as Ollama LLM

    Note over U, LLM: ðŸ“¥ Document Ingestion Flow
    U->>API: POST /ingest/upload (file)
    API->>IS: process_document(file)
    IS->>IS: load â†’ split â†’ embed
    IS->>VS: store(chunks + vectors)
    VS-->>API: âœ… stored
    API-->>U: 200 OK (doc_id, chunk_count)

    Note over U, LLM: ðŸ” Query & Generation Flow
    U->>API: POST /query/ask (question)
    API->>RS: query(question)
    RS->>RS: embed(question)
    RS->>VS: similarity_search(query_vector, top_k)
    VS-->>RS: relevant chunks
    RS->>RS: build_prompt(question, chunks)
    RS->>LLM: generate(prompt)
    LLM-->>RS: answer
    RS-->>API: answer + sources
    API-->>U: 200 OK (answer, citations)
```

---

## ðŸš€ Quick Start

### Prerequisites

| Dependency | Purpose | Install |
|---|---|---|
| **Ollama 3.2+** | Local LLM | [ollama.com](https://ollama.com/) |
| **Python 3.10+** | Runtime | [python.org](https://www.python.org/downloads/) |
| **Redis** | Vector store & cache | [redis.io](https://redis.io/download) |

### 1. Clone & Install

```bash
git clone https://github.com/shubhamauti9/retrieval-augmented-generation.git
cd retrieval-augmented-generation

# Create virtual environment
python -m venv venv
source venv/bin/activate   # Linux/macOS
venv\Scripts\activate      # Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. Start Infrastructure

```bash
# Start Ollama (Docker)
docker run -d --name ollama -p 11434:11434 ollama/ollama

# Start Redis (Docker)
docker run -d --name redis -p 6379:6379 redis:7-alpine
```

### 3. Run the API Server

```bash
uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload
```

The API will be available at:
- **Swagger UI** â†’ [http://localhost:8000/docs](http://localhost:8000/docs)
- **ReDoc** â†’ [http://localhost:8000/redoc](http://localhost:8000/redoc)

### 4. Try It Out

```bash
# Upload a document
curl -X POST http://localhost:8000/ingest/upload \
  -F "file=@your_document.pdf" \
  -F "collection=default"

# Ask a question
curl -X POST http://localhost:8000/query/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the main topic of the document?", "collection": "default"}'
```

---

## Project Structure

```
rag-from-scratch/
â”‚
â”œâ”€â”€ api/                              # FastAPI application layer
â”‚   â”œâ”€â”€ main.py                       # App entrypoint, middleware, routes
â”‚   â”œâ”€â”€ config.py                     # Settings via pydantic-settings (.env)
â”‚   â”œâ”€â”€ models.py                     # Pydantic request/response models
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ ingest.py                 # POST /ingest/upload, /ingest/text
â”‚   â”‚   â”œâ”€â”€ query.py                  # POST /query/ask, /query/search
â”‚   â”‚   â””â”€â”€ admin.py                  # GET /admin/health, collections mgmt
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ ingestion.py              # Document processing orchestration
â”‚       â””â”€â”€ retrieval.py              # Query processing & LLM orchestration
â”‚
â”œâ”€â”€ src/                              # Core RAG library (framework-agnostic)
â”‚   â”œâ”€â”€ loaders/                      # Document loaders
â”‚   â”‚   â”œâ”€â”€ base_loader.py            # Abstract base class
â”‚   â”‚   â”œâ”€â”€ pdf_loader.py             # PDF â†’ text (pypdf)
â”‚   â”‚   â”œâ”€â”€ docx_loader.py           # Word documents (python-docx)
â”‚   â”‚   â”œâ”€â”€ excel_loader.py           # Excel files (openpyxl)
â”‚   â”‚   â”œâ”€â”€ pptx_loader.py           # PowerPoint (python-pptx)
â”‚   â”‚   â”œâ”€â”€ text_loader.py            # Plain text / Markdown
â”‚   â”‚   â””â”€â”€ directory_loader.py       # Batch load from directory
â”‚   â”‚
â”‚   â”œâ”€â”€ text_splitters/               # Text chunking strategies
â”‚   â”‚   â”œâ”€â”€ recursive_character_text_splitter.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ embeddings/                   # Embedding generation & caching
â”‚   â”‚   â”œâ”€â”€ embedding_model.py        # Sentence-Transformers wrapper
â”‚   â”‚   â””â”€â”€ embedding_cache.py        # Cache layer for embeddings
â”‚   â”‚
â”‚   â”œâ”€â”€ vector_stores/                # Vector storage backends
â”‚   â”‚   â”œâ”€â”€ base_vector_store.py      # Abstract interface
â”‚   â”‚   â”œâ”€â”€ in_memory_vector_store.py # Development / testing store
â”‚   â”‚   â””â”€â”€ redis_vector_store.py     # Production Redis store
â”‚   â”‚
â”‚   â”œâ”€â”€ retrievers/                   # Retrieval strategies
â”‚   â”‚   â”œâ”€â”€ base_retriever.py         # Abstract retriever
â”‚   â”‚   â””â”€â”€ vector_store_retriever.py # Cosine similarity retrieval
â”‚   â”‚
â”‚   â”œâ”€â”€ chains/                       # Pipeline orchestration
â”‚   â”‚   â””â”€â”€ rag_chain.py              # Full RAG chain (retrieve â†’ prompt â†’ generate)
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/                      # Prompt engineering
â”‚   â”‚   â””â”€â”€ prompt_template.py        # Configurable prompt templates
â”‚   â”‚
â”‚   â”œâ”€â”€ cache/                        # Redis caching infrastructure
â”‚   â”‚   â”œâ”€â”€ redis_manager.py          # Redis connection management
â”‚   â”‚   â”œâ”€â”€ embedding_cache.py        # Embedding-level caching
â”‚   â”‚   â””â”€â”€ query_cache.py            # Query result caching
â”‚   â”‚
â”‚   â””â”€â”€ utils/                        # Shared utilities
â”‚       â”œâ”€â”€ document.py               # Document data class
â”‚       â””â”€â”€ similarity.py             # Cosine similarity functions
â”‚
â”œâ”€â”€ data/                             # Runtime data (gitignored)
â”œâ”€â”€ Dockerfile                        # Multi-stage production build
â”œâ”€â”€ pyproject.toml                    # Project metadata & dependencies
â”œâ”€â”€ requirements.txt                  # Pinned dependencies
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dockerignore
â””â”€â”€ README.md
```

---

## API Reference

### Ingestion Endpoints

| Method | Endpoint | Description |
|---|---|---|
| `POST` | `/ingest/upload` | Upload a file (PDF, DOCX, XLSX, PPTX, TXT, MD) |
| `POST` | `/ingest/text` | Ingest raw text directly |

### Query Endpoints

| Method | Endpoint | Description |
|---|---|---|
| `POST` | `/query/ask` | Ask a question and get an LLM-generated answer with sources |
| `POST` | `/query/search` | Semantic search â€” returns relevant chunks without generation |

### Admin Endpoints

| Method | Endpoint | Description |
|---|---|---|
| `GET` | `/admin/health` | Health check with system status |
| `GET` | `/admin/collections` | List all document collections |
| `DELETE` | `/admin/collections/{name}` | Delete a collection and its documents |

> Full interactive documentation available at `/docs` (Swagger UI) when the server is running.

---

## Configuration

All settings are managed via **environment variables** with the `RAG_` prefix, or through a `.env` file.

| Variable | Default | Description |
|---|---|---|
| `RAG_DEBUG` | `false` | Enable debug mode |
| `RAG_EMBEDDING_MODEL` | `all-MiniLM-L6-v2` | Sentence-Transformers model name |
| `RAG_EMBEDDING_DIM` | `384` | Embedding vector dimensions |
| `RAG_LLM_MODEL` | `llama3.2` | Ollama model name |
| `RAG_LLM_BASE_URL` | `http://localhost:11434` | Ollama server URL |
| `RAG_CHUNK_SIZE` | `500` | Max characters per chunk |
| `RAG_CHUNK_OVERLAP` | `100` | Overlap between consecutive chunks |
| `RAG_DEFAULT_TOP_K` | `5` | Number of chunks to retrieve |
| `RAG_REDIS_HOST` | `localhost` | Redis server host |
| `RAG_REDIS_PORT` | `6379` | Redis server port |
| `RAG_REDIS_PASSWORD` | â€” | Redis password (optional) |
| `RAG_REDIS_ENABLED` | `true` | Use Redis vs in-memory store |
| `RAG_EMBEDDING_CACHE_TTL` | `604800` | Embedding cache TTL (7 days) |
| `RAG_QUERY_CACHE_TTL` | `3600` | Query cache TTL (1 hour) |

#### Note: 
- `RAG_EMBEDDING_CACHE_TTL` and `RAG_QUERY_CACHE_TTL` can be customized based on your needs. For example, you can set `RAG_EMBEDDING_CACHE_TTL` to `0` to disable embedding cache, or set `RAG_QUERY_CACHE_TTL` to `0` to disable query cache
- 'RAG_CHUNK_SIZE' and 'RAG_CHUNK_OVERLAP' can be customized based on your needs. For example, you can set `RAG_CHUNK_SIZE` to `0` to disable chunking, or set `RAG_CHUNK_OVERLAP` to `0` to disable overlap

### Example `.env`

```env
RAG_DEBUG=true
RAG_LLM_MODEL=llama3.2
RAG_REDIS_HOST=redis
RAG_REDIS_PORT=6379
RAG_CHUNK_SIZE=500
RAG_DEFAULT_TOP_K=5
```

---

## Docker

### Build & Run

```bash
# Build the image
docker build -t rag-pipeline .

# Run the container
docker run -p 8000:8000 \
  -e RAG_REDIS_HOST=host.docker.internal \
  -e RAG_LLM_BASE_URL=http://host.docker.internal:11434 \
  rag-pipeline
```

### Docker Compose (Recommended)

Create a `docker-compose.yml` for the full stack:

```yaml
version: "3.9"

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - RAG_REDIS_HOST=redis
      - RAG_REDIS_PORT=6379
      - RAG_LLM_BASE_URL=http://host.docker.internal:11434
    depends_on:
      - redis

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

volumes:
  redis_data:
```

```bash
docker compose up -d
```

---

## How It Works â€” Step by Step

Understanding each stage of the RAG pipeline:

### 1. Document Loading
Documents are loaded using format-specific loaders that extract text and preserve metadata (filename, page numbers, sheet names).

### 2. Text Splitting
Long documents are split into overlapping chunks using `RecursiveCharacterTextSplitter`. The overlap ensures context is not lost at chunk boundaries.

### 3. Embedding Generation
Each chunk is converted into a 384-dimensional vector using the `all-MiniLM-L6-v2` model from Sentence-Transformers. Embeddings are cached in Redis to avoid redundant computation.

### 4. Vector Storage
Embeddings and their associated text chunks are stored in Redis (production) or an in-memory store (development). Redis uses sorted sets and hash maps for efficient similarity search.

### 5. Semantic Retrieval
When a query arrives, it is embedded using the same model. The vector store performs cosine similarity search to find the top-k most relevant chunks.

### 6. Prompt Construction
Retrieved chunks are injected into a structured prompt template that instructs the LLM to answer based only on the provided context.

### 7. LLM Generation
The prompt is sent to a LLM which generates a grounded answer with source citations.

---

## Development

```bash
# Install dev dependencies
pip install -e ".[dev]"

# Run tests
pytest --cov=src --cov-report=html

# Format code
black src/ api/

# Lint
ruff check src/ api/
```

---

## License

This project is licensed under the [MIT License](LICENSE).

---
